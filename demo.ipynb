{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from model.dali_pipe import dali_generator\n",
    "from model.resnet import Resnet50\n",
    "from model.lars import LARS\n",
    "from model.scheduler import WarmupExponentialDecay\n",
    "import horovod.tensorflow as hvd\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvd.init()\n",
    "\n",
    "data_dir = Path('/home/ubuntu/shared_workspace/data/imagenet/')\n",
    "index_dir = Path('/home/ubuntu/shared_workspace/data/imagenet_index/')\n",
    "train_files = [i.as_posix() for i in data_dir.glob('*1024')]\n",
    "train_index = [i.as_posix() for i in index_dir.glob('*1024')]\n",
    "\n",
    "num_epochs = 70\n",
    "global_batch = 512\n",
    "per_gpu_batch = global_batch//hvd.size()\n",
    "image_count = 1282048\n",
    "steps_per_epoch = image_count//global_batch\n",
    "learning_rate = 0.01*global_batch/256\n",
    "scaled_rate = 0.1*global_batch/256\n",
    "\n",
    "'''tf.keras.backend.set_floatx('float16')\n",
    "tf.keras.backend.set_epsilon(1e-4)'''\n",
    "tf.config.optimizer.set_jit(True)\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "if gpus:\n",
    "    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = WarmupExponentialDecay(learning_rate, \n",
    "                                   scaled_rate, steps_per_epoch*5, steps_per_epoch*30, 1e-5)\n",
    "train_tf = dali_generator(train_files, train_index, \n",
    "                          per_gpu_batch, num_threads=8, device_id=hvd.rank(), total_devices=hvd.size())\n",
    "model = tf.keras.applications.ResNet50(weights=None, input_shape=(224, 224, 3), classes=1000)\n",
    "optimizer = LARS(scheduler, use_nesterov=False, clip=False)\n",
    "optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, loss_scale=\"dynamic\")\n",
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(images, training=True)\n",
    "        loss = loss_func(labels, pred)\n",
    "        scaled_loss = optimizer.get_scaled_loss(loss)\n",
    "    scaled_grads = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "    grads = optimizer.get_unscaled_gradients(scaled_grads)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2504 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer conv1_pad is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_loss: 6.0646: 100%|██████████| 2504/2504 [19:57<00:00,  2.09it/s]\n",
      "train_loss: 5.3338:  67%|██████▋   | 1690/2504 [12:38<06:04,  2.23it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    loss = []\n",
    "    progressbar = tqdm(range(steps_per_epoch))\n",
    "    for batch in progressbar:\n",
    "        images, labels = next(train_tf)\n",
    "        loss.append(train_step(images, labels).numpy())\n",
    "        progressbar.set_description(\"train_loss: {0:.4f}\".format(np.array(loss[-100:]).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_step(images, labels):\n",
    "    with tf.device('/gpu:0'):\n",
    "        pred = model(images, training=False)\n",
    "        loss = loss_func(labels, pred)\n",
    "        top_5_pred = tf.math.top_k(pred, k=5)[1]\n",
    "        top_1_pred = tf.math.top_k(pred, k=1)[1]\n",
    "        labels = tf.cast(labels, tf.int32)\n",
    "    top_1 = sum([label in a_pred for label, a_pred in zip(labels, top_1_pred)])\n",
    "    top_5 = sum([label in a_pred for label, a_pred in zip(labels, top_5_pred)])\n",
    "    return loss, top_1, top_5\n",
    "\n",
    "def validation(steps = 128):\n",
    "    loss_tracker = []\n",
    "    top_1_tracker = 0\n",
    "    top_5_tracker = 0\n",
    "    for _ in range(steps):\n",
    "        images, labels = next(validation_tdf)\n",
    "        loss, top_1, top_5 = validation_step(images, labels)\n",
    "        loss_tracker.append(loss.numpy())\n",
    "        top_1_tracker += top_1\n",
    "        top_5_tracker += top_5\n",
    "    return sum(loss_tracker)/len(loss_tracker), top_1_tracker/(steps*batch_size), top_5_tracker/(steps*batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_files = [i.as_posix() for i in data_dir.glob('*0128')]\n",
    "validation_index = [i.as_posix() for i in index_dir.glob('*0128')]\n",
    "validation_tdf = dali_generator(validation_files, validation_index, batch_size, num_threads=4, device_id=hvd.rank(), total_devices=hvd.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.04437255859375, 0.00079345703125, 0.00439453125)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
